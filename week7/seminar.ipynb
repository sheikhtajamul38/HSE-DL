{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zriTdjauH8iQ"
      },
      "outputs": [],
      "source": [
        "%pip install -q transformers huggingface_hub\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQiRPWWHlSgv"
      },
      "source": [
        "### Using pre-trained transformers (2 points)\n",
        "_for fun and profit_\n",
        "\n",
        "There are many toolkits that let you access pre-trained transformer models, but the most powerful and convenient by far is [`huggingface/transformers`](https://github.com/huggingface/transformers). In this week's practice, you'll learn how to download, apply and modify pre-trained transformers for a range of tasks. Buckle up, we're going in!\n",
        "\n",
        "\n",
        "__Pipelines:__ if all you want is to apply a pre-trained model, you can do that in one line of code using pipeline. Huggingface/transformers has a selection of pre-configured pipelines for masked language modelling, sentiment classification, question aswering, etc. ([see full list here](https://huggingface.co/transformers/main_classes/pipelines.html))\n",
        "\n",
        "A typical pipeline includes:\n",
        "* pre-processing, e.g. tokenization, subword segmentation\n",
        "* a backbone model, e.g. bert finetuned for classification\n",
        "* output post-processing\n",
        "\n",
        "Let's see it in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rP1KFtvLlJHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "589133bf-1e1e-48be-ec0b-67bf47aae172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9998860359191895}]\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "classifier = transformers.pipeline('sentiment-analysis', model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "print(classifier(\"BERT is amazing!\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYUNuyXMn5l9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7e890d6-0c7c-4fb9-c64c-64558b4fa585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Well done!\n"
          ]
        }
      ],
      "source": [
        "import base64\n",
        "data = {\n",
        "    'arryn': 'As High as Honor.',\n",
        "    'baratheon': 'Ours is the fury.',\n",
        "    'stark': 'Winter is coming.',\n",
        "    'tyrell': 'Growing strong.'\n",
        "}\n",
        "\n",
        "# YOUR CODE: predict sentiment for each noble house and create outputs dict\n",
        "#<...>\n",
        "outputs = {house: classifier(motto)[0]['label'] == 'POSITIVE' for house, motto in data.items()}\n",
        "\n",
        "assert sum(outputs.values()) == 3 and outputs[base64.decodebytes(b'YmFyYXRoZW9u\\n').decode()] == False\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRDhIH-XpSNo"
      },
      "source": [
        "You can also access vanilla Masked Language Model that was trained to predict masked words. Here's how:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pa-8noIllRbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40e0343b-2d22-4a05-ff6b-c7e4493b6527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P=0.99719 donald trump is the president of the united states.\n",
            "P=0.00024 donald duck is the president of the united states.\n",
            "P=0.00022 donald ross is the president of the united states.\n",
            "P=0.00020 donald johnson is the president of the united states.\n",
            "P=0.00018 donald wilson is the president of the united states.\n"
          ]
        }
      ],
      "source": [
        "mlm_model = transformers.pipeline('fill-mask', model=\"bert-base-uncased\")\n",
        "MASK = mlm_model.tokenizer.mask_token\n",
        "\n",
        "for hypo in mlm_model(f\"Donald {MASK} is the president of the united states.\"):\n",
        "  print(f\"P={hypo['score']:.5f}\", hypo['sequence'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NxeG1Y5pwX1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "232fb266-ebc2-43bf-cd50-9bb27cb7515d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.16014553606510162,\n",
              "  'token': 3607,\n",
              "  'token_str': 'russia',\n",
              "  'sequence': 'the soviet union, formally known as the union of soviet socialist republics ( ussr ), was established in russia.'},\n",
              " {'score': 0.1354207992553711,\n",
              "  'token': 2255,\n",
              "  'token_str': 'october',\n",
              "  'sequence': 'the soviet union, formally known as the union of soviet socialist republics ( ussr ), was established in october.'},\n",
              " {'score': 0.05341670289635658,\n",
              "  'token': 4924,\n",
              "  'token_str': 'moscow',\n",
              "  'sequence': 'the soviet union, formally known as the union of soviet socialist republics ( ussr ), was established in moscow.'},\n",
              " {'score': 0.039383139461278915,\n",
              "  'token': 13125,\n",
              "  'token_str': 'stalin',\n",
              "  'sequence': 'the soviet union, formally known as the union of soviet socialist republics ( ussr ), was established in stalin.'},\n",
              " {'score': 0.0279727540910244,\n",
              "  'token': 2885,\n",
              "  'token_str': 'europe',\n",
              "  'sequence': 'the soviet union, formally known as the union of soviet socialist republics ( ussr ), was established in europe.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "# Your turn: use bert to recall what year was the Soviet Union founded in\n",
        "mlm_model(\"The Soviet Union, formally known as the Union of Soviet Socialist Republics (USSR), was established in [MASK].\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = f\"The Soviet Union was founded in {MASK}.\"\n",
        "for hypo in mlm_model(prompt):\n",
        "  print(f\"P={hypo['score']:.5f}\", hypo['sequence'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROc7LyWcGrcS",
        "outputId": "48d96fb2-4d26-42c2-b231-582faf146cb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P=0.29111 the soviet union was founded in russia.\n",
            "P=0.09198 the soviet union was founded in europe.\n",
            "P=0.03760 the soviet union was founded in moscow.\n",
            "P=0.02844 the soviet union was founded in october.\n",
            "P=0.02342 the soviet union was founded in siberia.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJxRFzCSq903"
      },
      "source": [
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Huggingface offers hundreds of pre-trained models that specialize on different tasks. You can quickly find the model you need using [this list](https://huggingface.co/models).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRux8Qp2hkXr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7455fc8f-748f-4d29-fb22-58e0c0f76c8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"Almost two-thirds of the 1.5 million people who viewed this liveblog had Googled to discover\n",
        " the latest on the Rosetta mission. They were treated to this detailed account by the Guardian’s science editor,\n",
        " Ian Sample, and astronomy writer Stuart Clark of the moment scientists landed a robotic spacecraft on a comet\n",
        " for the first time in history, and the delirious reaction it provoked at their headquarters in Germany.\n",
        "  “We are there. We are sitting on the surface. Philae is talking to us,” said one scientist.\n",
        "\"\"\"\n",
        "\n",
        "# Task: create a pipeline for named entity recognition, use task name 'ner' and search for the right model in the list\n",
        "ner_model =transformers.pipeline('ner', model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")#<YOUR CODE>\n",
        "\n",
        "\n",
        "named_entities = ner_model(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf57MRzSiSON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a515b259-5084-460a-a3e9-9f294e05e379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OUTPUT: [{'entity': 'I-MISC', 'score': 0.88031083, 'index': 19, 'word': 'Google', 'start': 73, 'end': 79}, {'entity': 'I-MISC', 'score': 0.9005079, 'index': 27, 'word': 'Rose', 'start': 112, 'end': 116}, {'entity': 'I-MISC', 'score': 0.95096296, 'index': 28, 'word': '##tta', 'start': 116, 'end': 119}, {'entity': 'I-ORG', 'score': 0.99925345, 'index': 40, 'word': 'Guardian', 'start': 179, 'end': 187}, {'entity': 'I-PER', 'score': 0.999201, 'index': 46, 'word': 'Ian', 'start': 207, 'end': 210}, {'entity': 'I-PER', 'score': 0.9994999, 'index': 47, 'word': 'Sam', 'start': 211, 'end': 214}, {'entity': 'I-PER', 'score': 0.99649787, 'index': 48, 'word': '##ple', 'start': 214, 'end': 217}, {'entity': 'I-PER', 'score': 0.9991856, 'index': 53, 'word': 'Stuart', 'start': 240, 'end': 246}, {'entity': 'I-PER', 'score': 0.99964833, 'index': 54, 'word': 'Clark', 'start': 247, 'end': 252}, {'entity': 'I-LOC', 'score': 0.9998211, 'index': 85, 'word': 'Germany', 'start': 413, 'end': 420}, {'entity': 'I-PER', 'score': 0.62957036, 'index': 99, 'word': 'Phil', 'start': 470, 'end': 474}, {'entity': 'I-PER', 'score': 0.8340382, 'index': 100, 'word': '##ae', 'start': 474, 'end': 476}]\n",
            "All tests passed\n"
          ]
        }
      ],
      "source": [
        "print('OUTPUT:', named_entities)\n",
        "word_to_entity = {item['word']: item['entity'] for item in named_entities}\n",
        "assert 'org' in word_to_entity.get('Guardian').lower() and 'per' in word_to_entity.get('Stuart').lower()\n",
        "print(\"All tests passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULMownz6sP9n"
      },
      "source": [
        "### The building blocks of a pipeline\n",
        "\n",
        "Huggingface also allows you to access its pipelines on a lower level. There are two main abstractions for you:\n",
        "* `Tokenizer` - converts from strings to token ids and back\n",
        "* `Model` - a pytorch `nn.Module` with pre-trained weights\n",
        "\n",
        "You can use such models as part of your regular pytorch code: insert is as a layer in your model, apply it to a batch of data, backpropagate, optimize, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMJbV0QVsO0Q"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = transformers.AutoModel.from_pretrained('bert-base-uncased')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgSPHKPRxG6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d729f7d1-8090-4181-d6a1-d8ff22fb014e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids tensor([[ 101, 5355, 1010, 1045, 2572, 2115, 2269, 1012,  102,    0,    0,    0,\n",
            "            0,    0,    0],\n",
            "        [ 101, 2166, 2003, 2054, 6433, 2043, 2017, 1005, 2128, 5697, 2437, 2060,\n",
            "         3488, 1012,  102]])\n",
            "token_type_ids tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
            "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "Detokenized:\n",
            "[CLS] luke, i am your father. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "[CLS] life is what happens when you're busy making other plans. [SEP]\n"
          ]
        }
      ],
      "source": [
        "lines = [\n",
        "    \"Luke, I am your father.\",\n",
        "    \"Life is what happens when you're busy making other plans.\",\n",
        "    ]\n",
        "\n",
        "# tokenize a batch of inputs. \"pt\" means [p]y[t]orch tensors\n",
        "tokens_info = tokenizer(lines, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "for key in tokens_info:\n",
        "    print(key, tokens_info[key])\n",
        "\n",
        "print(\"Detokenized:\")\n",
        "for i in range(2):\n",
        "    print(tokenizer.decode(tokens_info['input_ids'][i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJkbHxERyfL4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c355346d-ffd9-40c3-e150-8c5650795188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.8854, -0.4722, -0.9392,  ..., -0.8081, -0.6955,  0.8748],\n",
            "        [-0.9297, -0.5161, -0.9334,  ..., -0.9017, -0.7492,  0.9201]])\n"
          ]
        }
      ],
      "source": [
        "# You can now apply the model to get embeddings\n",
        "with torch.no_grad():\n",
        "    out = model(**tokens_info)\n",
        "\n",
        "print(out['pooler_output'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Vij7Gc1wOaq"
      },
      "source": [
        "Transformers knowledge hub: https://huggingface.co/transformers/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwmTTyjUGqol"
      },
      "source": [
        "### Build-a-transformer (2 points)\n",
        "\n",
        "In this section, you will implement a transformer language model layer by layer, then use it to generate (hopefully) coherent text.\n",
        "\n",
        "To understand how these layers work, please check out our guide to transformers from [nlp course for you -> transformers](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#transformer_intro).\n",
        "\n",
        "\n",
        "First, we download pre-trained weights for the [GPT2 model by OpenAI](https://openai.com/research/better-language-models) - a prominent model from 2019.\n",
        "\n",
        "\n",
        "\n",
        "Idea & code by: Ilya Beletsky"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOcK0lGTGqol",
        "outputId": "791ac469-5c6d-4dab-80ab-747648d85548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: ['h.0.attn.c_attn.bias', 'h.0.attn.c_attn.weight', 'h.0.attn.c_proj.bias', 'h.0.attn.c_proj.weight', 'h.0.ln_1.bias', 'h.0.ln_1.weight', 'h.0.ln_2.bias', 'h.0.ln_2.weight', 'h.0.mlp.c_fc.bias', 'h.0.mlp.c_fc.weight', 'h.0.mlp.c_proj.bias', 'h.0.mlp.c_proj.weight', 'h.1.attn.c_attn.bias', 'h.1.attn.c_attn.weight', 'h.1. ...\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "state_dict = torch.load(hf_hub_download(\"gpt2\", filename=\"pytorch_model.bin\"))\n",
        "for key, value in tuple(state_dict.items()):\n",
        "    if key.startswith('h.') and key.endswith('.weight') and value.ndim == 2:\n",
        "        value.transpose_(1, 0)  # <-- for compatibility with modern PyTorch modules\n",
        "    if key.startswith('h.') and key.endswith('.attn.bias') and value.ndim == 4:\n",
        "        state_dict.pop(key)  # <-- triangular binar masks, not needed in this code\n",
        "\n",
        "print('Weights:', repr(sorted(state_dict.keys()))[:320], '...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr0SUtQnGqom"
      },
      "source": [
        "In the next few cells, we shall implement the model layer by layer to make use of those weights.\n",
        "\n",
        "As you might recall, transformers contain two main layer types: attention and fully-connected layers.\n",
        "\n",
        "The fully connected layers are by far easier to understand, so we shall begin there:\n",
        "\n",
        "Please implement fully-connected layer __without residual or layer normalization__ (we'll add those in a bit)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Rh-6DX9Gqom"
      },
      "outputs": [],
      "source": [
        "class GeLUThatWasUsedInGPT2(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * x ** 3)))\n",
        "\n",
        "class FullyConnected(nn.Module):\n",
        "    def __init__(self, dim: int):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(dim, 4  * dim)\n",
        "        self.gelu = GeLUThatWasUsedInGPT2()\n",
        "        self.c_proj = nn.Linear(4 * dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # x.shape = [batch_size, seq_length, dim]\n",
        "      x = self.c_fc(x)\n",
        "      x = self.gelu(x)\n",
        "      x = self.c_proj(x)\n",
        "      return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSVGKnHBGqom"
      },
      "source": [
        "Now, let's test that it works with GPT-2 weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoWjZwZkGqom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c774bcad-54f2-48e4-8a38-a22d197c1899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seems legit!\n"
          ]
        }
      ],
      "source": [
        "mlp = FullyConnected(dim=768)\n",
        "mlp.load_state_dict({'c_fc.weight': state_dict['h.0.mlp.c_fc.weight'],\n",
        "                     'c_fc.bias': state_dict['h.0.mlp.c_fc.bias'],\n",
        "                     'c_proj.weight': state_dict['h.0.mlp.c_proj.weight'],\n",
        "                     'c_proj.bias': state_dict['h.0.mlp.c_proj.bias']})\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "x = torch.randn(1, 2, 768)  # [batch_size, sequence_length, dim]\n",
        "checksum = torch.sum(mlp(x) * x)\n",
        "\n",
        "assert abs(checksum.item() - 1282.3315) < 0.1, \"layer outputs do not match reference\"\n",
        "assert torch.allclose(mlp(x[:, (1, 0), :])[:, (1, 0), :], mlp(x)), \"mlp must be permutation-invariant\"\n",
        "print(\"Seems legit!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbfCevRwGqom"
      },
      "source": [
        "Now, let's get to attention layers.\n",
        "\n",
        "Since GPT-2 needs to generate text from left to right, each generated token can only attend to tokens on the left (and itself). This kid of attention is called \"Masked\" self-attention, because it hides tokens to the right.\n",
        "\n",
        "As before, please implement masked self-attention __without layernorm or residual connections.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6j7M4hLGqon"
      },
      "outputs": [],
      "source": [
        "\"\"\"class MaskedSelfAttention(nn.Module):\n",
        "    def __init__(self, dim: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        self.c_attn = nn.Linear(dim, dim * 3)  # query + key + value, combined\n",
        "        self.c_proj = nn.Linear(dim, dim)  # output projection\n",
        "        self.dim, self.num_heads = dim, num_heads\n",
        "        self.head_size = dim // num_heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        q, k, v = self.c_attn(x).split(dim=-1, split_size=self.dim)\n",
        "        assert q.shape == k.shape == v.shape == x.shape, \"q, k and v must have the same shape as x\"\n",
        "\n",
        "\n",
        "        # Note: this is an inefficient implementation that uses a for-loop.\n",
        "        # For bonus points, re-write the attention code below manually, with the following constrains:\n",
        "        # 1) do not use for-loops (or other loops). Compute everything in parallel with vectorized operations\n",
        "        # 2) do not use F.scaled_dot_product_attention - write your own attention code using basic PyTorch ops\n",
        "        head_outputs = []\n",
        "        for head_index in range(self.num_heads):\n",
        "            head_selector = range(self.head_size * head_index, self.head_size * (head_index + 1))\n",
        "\n",
        "            head_queries = q[..., head_selector]\n",
        "            head_keys = k[..., head_selector]\n",
        "            head_values = v[..., head_selector]\n",
        "\n",
        "            single_head_output = F.scaled_dot_product_attention(\n",
        "                <YOUR CODE HERE - fill in the missing parameters; see docs below>\n",
        "                is_causal=True)\n",
        "            # docs: https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "            head_outputs.append(single_head_output)\n",
        "\n",
        "        combined_head_outputs = torch.cat(head_outputs, dim=-1)\n",
        "        return self.c_proj(combined_head_outputs)\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class MaskedSelfAttention(nn.Module):\n",
        "    def __init__(self, dim: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        self.c_attn = nn.Linear(dim, dim * 3)  # query + key + value, combined\n",
        "        self.c_proj = nn.Linear(dim, dim)  # output projection\n",
        "        self.dim, self.num_heads = dim, num_heads\n",
        "        self.head_size = dim // num_heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        q, k, v = self.c_attn(x).split(split_size=self.dim, dim=-1)\n",
        "        assert q.shape == k.shape == v.shape == x.shape, \"q, k and v must have the same shape as x\"\n",
        "\n",
        "        # Reshape and permute the Q, K and V matrices to [Batch, Head, Seq, Feature]\n",
        "        q = q.view(x.shape[0], x.shape[1], self.num_heads, self.head_size).permute(0, 2, 1, 3)\n",
        "        k = k.view(x.shape[0], x.shape[1], self.num_heads, self.head_size).permute(0, 2, 3, 1)\n",
        "        v = v.view(x.shape[0], x.shape[1], self.num_heads, self.head_size).permute(0, 2, 1, 3)\n",
        "\n",
        "        scale = math.sqrt(self.head_size)\n",
        "        scores = torch.matmul(q, k) / scale\n",
        "\n",
        "        # Masking the upper triangle of the score matrix, including the diagonal.\n",
        "        # The diagonal is masked because the ith query should not attend to the ith key\n",
        "        # This creates a causal mask which is replicated across the batch and head dimensions.\n",
        "        causal_mask = torch.triu(torch.ones(scores.shape[-2:], device=scores.device), diagonal=1).bool()\n",
        "        scores.masked_fill_(causal_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
        "\n",
        "        # Apply softmax to the masked scores\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.permute(0, 2, 1, 3).contiguous().view(x.shape)\n",
        "        return self.c_proj(out)\n"
      ],
      "metadata": {
        "id": "3unOsxIDOCJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test that it works"
      ],
      "metadata": {
        "id": "umZpcpIkJva7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn = MaskedSelfAttention(dim=768, num_heads=12)\n",
        "attn.load_state_dict({'c_attn.weight': state_dict['h.0.attn.c_attn.weight'],\n",
        "                      'c_attn.bias': state_dict['h.0.attn.c_attn.bias'],\n",
        "                      'c_proj.weight': state_dict['h.0.attn.c_proj.weight'],\n",
        "                      'c_proj.bias': state_dict['h.0.attn.c_proj.bias']})\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "x = torch.randn(1, 10, 768)  # [batch_size, sequence_length, dim]\n",
        "checksum = torch.sum(attn(x) * x)\n",
        "assert abs(checksum.item() - 2703.6772) < 0.1, \"layer outputs do not match reference\"\n",
        "assert not torch.allclose(attn(x[:, (1, 0), :])[:, (1, 0), :], attn(x[:, (0, 1), :])), \"masked attention must *not* be permutation-invariant\"\n",
        "print(\"It works!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg5Oj_PPM6hj",
        "outputId": "323aeda3-3817-4a33-a049-701b6008f644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It works!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now combine attention and MLP to build the full transformer layer:\n",
        "\n",
        "![img](https://i.imgur.com/1sq2vHO.png)"
      ],
      "metadata": {
        "id": "rn6tgTHzOK4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, dim: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(dim)\n",
        "        self.attn = MaskedSelfAttention(dim, num_heads)\n",
        "        self.ln_2 = nn.LayerNorm(dim)\n",
        "        self.mlp = FullyConnected(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_attn = self.ln_1(x)\n",
        "        x_attn = self.attn(x_attn)\n",
        "        x = x + x_attn\n",
        "\n",
        "        x_mlp = self.ln_2(x)\n",
        "        x_mlp = self.mlp(x_mlp)\n",
        "        x = x + x_mlp\n",
        "        return x"
      ],
      "metadata": {
        "id": "p3AH7YQvRpvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = TransformerLayer(dim=768, num_heads=12)\n",
        "layer.load_state_dict({k[5:]: v for k, v in state_dict.items() if k.startswith('h.10.')})\n",
        "assert abs(torch.sum(layer(x) * x).item() - 9874.7383) < 0.1\n",
        "print(\"Good job!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qzo_QeFVSNZa",
        "outputId": "f3323a6d-6d57-4e65-9352-84bf9a65c459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good job!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2(nn.Module):\n",
        "    def __init__(self, vocab_size: int, dim: int, num_heads: int, num_layers: int, max_position_embeddings: int = 1024):\n",
        "        super().__init__()\n",
        "        self.wte = nn.Embedding(vocab_size, dim)  # token embeddings\n",
        "        self.wpe = nn.Embedding(max_position_embeddings, dim)  # position embeddings\n",
        "        self.ln_f = nn.LayerNorm(dim)   # final layer norm - goes after all transformer layers, but before logits\n",
        "\n",
        "        self.h = nn.Sequential(*(TransformerLayer(dim, num_heads) for layer in range(num_layers)))\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # input_ids.shape: [batch_size, sequence_length], int64 token ids\n",
        "        position_ids = torch.arange(input_ids.shape[1], device=input_ids.device).unsqueeze(0)\n",
        "\n",
        "        token_embeddings = self.wte(input_ids)\n",
        "        position_embeddings = self.wpe(position_ids)\n",
        "        full_embeddings = token_embeddings + position_embeddings\n",
        "\n",
        "        transformer_output = self.h(full_embeddings)\n",
        "        transformer_output_ln = self.ln_f(transformer_output)\n",
        "\n",
        "        # final layer: we predict logits by re-using token embeddings as linear weights\n",
        "        output_logits = transformer_output_ln @ self.wte.weight.T\n",
        "        return output_logits\n"
      ],
      "metadata": {
        "id": "Mbqw9iuaSrYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2', add_prefix_space=True)\n",
        "model = GPT2(vocab_size=50257, dim=768, num_heads=12, num_layers=12)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "input_ids = tokenizer(\"A quick\", return_tensors='pt')['input_ids']\n",
        "\n",
        "predicted_logits = model(input_ids)\n",
        "most_likely_token_id = predicted_logits[:, -1].argmax().item()\n",
        "\n",
        "print(\"Prediction:\", tokenizer.decode(most_likely_token_id))"
      ],
      "metadata": {
        "id": "p0m8jt66aDIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "839d0e19-c7e3-4818-c833-659b86ccc9a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction:  look\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The Fermi paradox \"\n",
        "tokens = tokenizer.encode(text)\n",
        "print(end=tokenizer.decode(tokens))\n",
        "line_length = len(tokenizer.decode(tokens))\n",
        "\n",
        "for i in range(500):\n",
        "    # Predict logits with your model\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.as_tensor([tokens]))\n",
        "\n",
        "    # Sample with probabilities\n",
        "    p_next = torch.softmax(logits[0, -1, :], dim=-1).data.cpu().numpy()\n",
        "    next_token_index = np.random.choice(len(p_next), p=p_next)\n",
        "\n",
        "    tokens.append(int(next_token_index))\n",
        "    print(end=tokenizer.decode(tokens[-1]))\n",
        "    line_length += len(tokenizer.decode(tokens[-1]))\n",
        "    if line_length > 120:\n",
        "      line_length = 0\n",
        "      print()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8ql3Lo7dXZ2",
        "outputId": "439a5f40-bcbd-498f-bf09-4a1ae4a003e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The Fermi paradox  looks somewhat like a Python functional structure.\n",
            "In general, the most pervasive common convention governing\n",
            " languages is the enigmatic Q 1 pattern (\"Q is \"). By determining a central type pair using integral literals, a theory of\n",
            " object identity, and intents, any type pair inferred from variadic libraries is intuitively guessed as a constituent part\n",
            " of an entity. This makes sense if the code or a parameter is set by the source code itself to a type, but it makes no sense\n",
            " if the programmer follows the own behavior that the functions for the given type are expected to follow.\n",
            "It turns out that\n",
            " an operation or parameter is actually a type, like the U+001 type pair that would be required to avoid LiD etc. Since members\n",
            " of a type set are sensed as high code coverage to others, this general rule seems to apply to the philosophies of other name\n",
            " associations and oh so immutable concrete classes.\n",
            "…Cool\n",
            "Hawabian Noise\n",
            "Here comes a stunning Zilog twist on classical scal\n",
            "ar languages whose large set of operators is strictly monoscedenced. As introducing deeper definition, I must realise that\n",
            " Fortran is currently a very good example (or perhaps better than I could ever possibly imagine), even a many man ranked there\n",
            " would have recognized when read this article. This is because, in general, Fortran is COAP x x-OFF Quake format match. As\n",
            " such, arrays are quicker and more performant than expected while indexes are far more composable via subobjects or mixed\n",
            " data types. In addition to being closely linked to a subset of neighbouring Open variables, arrays and some types combine\n",
            " to provide higher performance in polynomial time and conversely single threaded mechanism applications that cases like reads\n",
            "/javascripts and so on could benefit. The lib bytecode just selects arrays at the cost of an attempt at depth applying optimizations\n",
            " through its nested implementations or in combination with eyebrow MuT steps in considering an elements from multiple arrays\n",
            " (thus letting packages and libraries write incremental sequences you associate them with).\n",
            "Tricks of programming\n",
            "Even after\n",
            " a short time spent searching for a program that actually works, of course there is absolutely no guarantee you ever build\n",
            " some computer. That is another thing I learned in my 35 sixth iteration... and others that I learned is true out of real\n",
            " life. In fact they are many times more powerful than expected and still exotic to find in practice... but it is a subset\n",
            " that most people can win, for at least three reasons for given.\n",
            "Address"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3NJ0ocgGqop"
      },
      "source": [
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "### Here's how you can do the same with transformers library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTOHu124Gqop",
        "outputId": "df2b0eb4-4b81-48fb-e791-bfad599fde00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:  The Fermi paradox  is that the system's current theory of relativity does not work for all cases: the Fermi paradox may hold for the vast majority of quantum phenomena. Although some physicists have argued repeatedly that quantum physics does not hold for all cases, this idea\n"
          ]
        }
      ],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2', add_prefix_space=True)\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "print('Generated text:', tokenizer.decode(\n",
        "    model.generate(\n",
        "        **tokenizer(\"The Fermi paradox \", return_tensors='pt'),\n",
        "        do_sample=True, max_new_tokens=50\n",
        "    ).flatten().numpy()\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jtcHOz8KN8i1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py38",
      "language": "python",
      "name": "py38"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}